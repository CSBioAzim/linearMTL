% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/validate_TGGLMix.R
\name{RunTGGLMix}
\alias{RunTGGLMix}
\title{Convenience function for running a tree-guided group lasso mixture model
model (TGGLMix) multiple times.}
\usage{
RunTGGLMix(X = NULL, task.specific.features = list(), Y, M,
  test.ids = NULL, num.starts = 1, num.threads = NULL, groups, weights,
  lambda, verbose = 0, gam = 1, homoscedastic = FALSE,
  EM.max.iter = 200, EM.epsilon = 1e-05, EM.verbose = 0,
  sample.data = FALSE, TGGL.mu = 1e-05, TGGL.epsilon = 1e-05,
  shrink.mu = TRUE)
}
\arguments{
\item{X}{N by J1 matrix of features common to all tasks.}

\item{task.specific.features}{List of features which are specific to each
task. Each entry contains an N by J2 matrix for one particular task (where
columns are features). List has to be ordered according to the columns of
Y.}

\item{Y}{N by K output matrix for every task.}

\item{M}{Number of Clusters.}

\item{test.ids}{(Optional) Indices of data points to be used for testing.
Default is to use all data for training.}

\item{num.starts}{(Optional) Number of starts. Default is 1 (no restarts).}

\item{num.threads}{(Optional) Number of threads to be used. Default is 1.}

\item{groups}{Binary V by K matrix determining group membership: Task k in
group v iff groups[v,k] == 1.}

\item{weights}{V dimensional vector with group weights.}

\item{lambda}{Regularization parameter.}

\item{verbose}{(Optional) Integer in {0,1,2}. verbose = 0: No output.
verbose = 1: Print summary at the end of the optimization. verbose = 2:
Print progress during optimization.}

\item{gam}{(Optional) Regularization parameter for component m will be lambda
times the prior for component m to the power of gam.}

\item{homoscedastic}{(Optional) Force variance to be the same for all tasks
in a component. Default is FALSE.}

\item{EM.max.iter}{(Optional) Maximum number of iterations for EM algorithm.}

\item{EM.epsilon}{(Optional) Desired accuracy. Algorithm will terminate if
change in penalized negative log-likelihood drops below EM.epsilon.}

\item{EM.verbose}{(Optional) Integer in {0,1,2}. verbose = 0: No output.
verbose = 1: Print summary at the end of the optimization. verbose = 2:
Print progress during optimization.}

\item{sample.data}{(Optional) Sample data according to posterior probability
or not.}

\item{TGGL.mu}{(Optional) Mu parameter for TGGL.}

\item{TGGL.epsilon}{(Optional) Epsilon parameter for TGGL.}

\item{shrink.mu}{(Optional) Multiply mu by min(lambda, 1).}
}
\value{
List containing
\item{models}{List of TGGL models for each component.}
\item{posterior}{N by M Matrix containing posterior probabilities.}
\item{prior}{Vector with prior probabilities for each component.}
\item{sigmas}{M by K Matrix with standard deviations for each component.}
\item{obj}{Penalized negative likelihood (final objective value).}
\item{train.loglik}{Likelihood for training data.}
\item{test.loglik}{Likelihood for test data.}
}
\description{
Fit a tree-guided group lasso mixture model. Restart with different random
initializations and keep the model with the lowest objective value. Optional:
Evaluate best model on test set. By default all data is used for training. If
test.ids is not NULL, exclude corresponding indices from training and use
them for testing instead.
}
\seealso{
\code{\link{TGGLMix}}
}
