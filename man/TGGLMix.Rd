% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tggl_mixture.R
\name{TGGLMix}
\alias{TGGLMix}
\title{Fit a tree-guided group lasso mixture model model (TGGLMix).}
\usage{
TGGLMix(X = NULL, task.specific.features = list(), Y, M, groups, weights,
  lambda, gam = 1, homoscedastic = FALSE, EM.max.iter = 1000,
  EM.epsilon = 1e-04, EM.verbose = 0, sample.data = FALSE,
  TGGL.mu = 1e-05, TGGL.epsilon = 1e-05, TGGL.iter = 25)
}
\arguments{
\item{X}{N by J1 matrix of features common to all tasks.}

\item{task.specific.features}{List of features which are specific to each
task. Each entry contains an N by J2 matrix for one particular task (where
columns are features). List has to be ordered according to the columns of
Y.}

\item{Y}{N by K output matrix for every task.}

\item{M}{Number of Clusters.}

\item{groups}{Binary V by K matrix determining group membership: Task k in
group v iff groups[v,k] == 1.}

\item{weights}{V dimensional vector with group weights.}

\item{lambda}{Regularization parameter.}

\item{gam}{(Optional) Regularization parameter for component m will be lambda
times the prior for component m to the power of gam.}

\item{homoscedastic}{(Optional) Force variance to be the same for all tasks
in a component. Default is FALSE.}

\item{EM.max.iter}{(Optional) Maximum number of iterations for EM algorithm.}

\item{EM.epsilon}{(Optional) Desired accuracy. Algorithm will terminate if
change in penalized negative log-likelihood drops below EM.epsilon.}

\item{EM.verbose}{(Optional) Integer in {0,1,2}. verbose = 0: No output.
verbose = 1: Print summary at the end of the optimization. verbose = 2:
Print progress during optimization.}

\item{sample.data}{(Optional) Sample data according to posterior probability
or not.}

\item{TGGL.mu}{(Optional) Mu parameter for TGGL.}

\item{TGGL.epsilon}{(Optional) Epsilon parameter for TGGL.}

\item{TGGL.iter}{(Optional) Initial number of iterations for TGGL. Will be
increased incrementally to ensure convergence. When the number of samples
is much larger than the dimensionalty, it can be beneficial to use a large
initial number of iterations for TGGL. This is because every run of TGGL
requires precomputation of multiple n-by-n matrix products.}
}
\value{
List containing
\item{models}{List of TGGL models for each component.}
\item{posterior}{N by M Matrix containing posterior probabilities.}
\item{prior}{Vector with prior probabilities for each component.}
\item{sigmas}{M by K Matrix with standard deviations for each component.}
\item{obj}{Penalized negative log-likelihood (final objective value).}
\item{loglik}{Likelihood for training data.}
\item{groups}{groups argument.}
\item{weights}{weights argument.}
\item{lambda}{lambda argument.}
}
\description{
Fit a tree-guided group lasso mixture model using a generalized EM
algorithm. May be trained on shared or task specific feature matrices.
}
\seealso{
\code{\link{TreeGuidedGroupLasso}}
}
